{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/unclecode/crawl4ai.git /content/crawl4ai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVQ6V92CZYgS",
        "outputId": "d8426d0e-85f5-47e6-a0a6-ca79e6471830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/content/crawl4ai' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/crawl4ai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wv7q5XUZn0I",
        "outputId": "425d1f32-3c96-43d5-9030-3c317518a4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/crawl4ai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjm1B8TbY7gJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import os\n",
        "\n",
        "# COLE SEU TOKEN AQUI (entre aspas)\n",
        "HF_TOKEN = \"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\"  # ex: hf_abc123...\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN  # opcional, se quiser usar env var\n",
        "\n",
        "client = InferenceClient(\n",
        "    \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "    token=HF_TOKEN,\n",
        ")\n",
        "print(\"Token carregado (prefixo):\", HF_TOKEN[:10])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUPPb5kjY9oE",
        "outputId": "6ddfa9ce-3cfc-4f65-ed2a-8872c7e767a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token carregado (prefixo): hf_qUqsxAP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def qwen_chat(system_prompt, user_prompt, max_tokens=1024, temperature=0.2):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": user_prompt},\n",
        "    ]\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "            messages=messages,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(\"❌ Erro chamando o modelo:\", e)\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "hFg2ygTqZH_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"Você é um arquiteto de software sênior.\"\n",
        "user_prompt = \"Explique em 3 tópicos o que é arquitetura em camadas.\"\n",
        "\n",
        "print(qwen_chat(system_prompt, user_prompt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLrMvRRwcFo7",
        "outputId": "aaa24418-9a13-4b50-dbfd-d8f2d2e7684f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Definição: A arquitetura em camadas, também conhecida como arquitetura n-tier ou arquitetura multi-nível, é uma abordagem de design de sistemas onde a aplicação é dividida em várias camadas, cada uma com uma função específica e interagindo com as outras através de interfaces bem definidas.\n",
            "\n",
            "2. Benefícios: Esta abordagem oferece vários benefícios. Em primeiro lugar, facilita a manutenção e escala da aplicação, pois cada camada pode ser desenvolvida, testada e atualizada independentemente das outras. Além disso, promove a segurança, já que os dados sensíveis são geralmente armazenados na camada mais interna do sistema. Por fim, permite uma melhor separação de responsabilidades, tornando o código mais organizado e fácil de entender.\n",
            "\n",
            "3. Estrutura: A arquitetura em camadas típicamente inclui três camadas principais - Apresentação (UI), Negócios (Business Logic) e Dados (Data Access). A UI é responsável por receber e apresentar informações ao usuário final. A camada de negócios contém todas as regras de negócio e lógica de processamento. E a camada de dados se encarrega de manipular e armazenar os dados. No entanto, esta estrutura pode variar dependendo do tipo de aplicação e das necessidades específicas do projeto.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "REPO_URL = \"https://github.com/unclecode/crawl4ai.git\"\n",
        "REPO_DIR = \"/content/crawl4ai\"\n",
        "\n",
        "# Change to a safe directory before potentially deleting the current one\n",
        "os.chdir('/content')\n",
        "\n",
        "# Recomeçar limpo\n",
        "if os.path.exists(REPO_DIR):\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "\n",
        "!git clone {REPO_URL} {REPO_DIR}\n",
        "os.chdir(REPO_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rjt2M_HZJfl",
        "outputId": "9750bcd9-5ac9-49ff-8f86-06e5ed07b10f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/crawl4ai'...\n",
            "remote: Enumerating objects: 10157, done.\u001b[K\n",
            "remote: Counting objects: 100% (2492/2492), done.\u001b[K\n",
            "remote: Compressing objects: 100% (529/529), done.\u001b[K\n",
            "remote: Total 10157 (delta 2250), reused 1963 (delta 1963), pack-reused 7665 (from 1)\u001b[K\n",
            "Receiving objects: 100% (10157/10157), 144.05 MiB | 21.06 MiB/s, done.\n",
            "Resolving deltas: 100% (6529/6529), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_files = []\n",
        "\n",
        "for root, dirs, files in os.walk(\"crawl4ai\"):\n",
        "    # se quiser ignorar subpastas específicas, coloque aqui\n",
        "    if any(skip in root for skip in [\"__pycache__\"]):\n",
        "        continue\n",
        "\n",
        "    for f in files:\n",
        "        if f.endswith(\".py\"):\n",
        "            code_files.append(os.path.join(root, f))\n",
        "\n",
        "len(code_files), code_files[:15]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFia-TA8ZKRp",
        "outputId": "5b9438f4-6349-4fac-c4f6-3b55b0fff021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(74,\n",
              " ['crawl4ai/user_agent_generator.py',\n",
              "  'crawl4ai/cli.py',\n",
              "  'crawl4ai/table_extraction.py',\n",
              "  'crawl4ai/browser_manager.py',\n",
              "  'crawl4ai/async_crawler_strategy.py',\n",
              "  'crawl4ai/async_url_seeder.py',\n",
              "  'crawl4ai/async_configs.py',\n",
              "  'crawl4ai/utils.py',\n",
              "  'crawl4ai/model_loader.py',\n",
              "  'crawl4ai/async_database.py',\n",
              "  'crawl4ai/proxy_strategy.py',\n",
              "  'crawl4ai/adaptive_crawler.py',\n",
              "  'crawl4ai/adaptive_crawler copy.py',\n",
              "  'crawl4ai/hub.py',\n",
              "  'crawl4ai/models.py'])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_list_preview = \"\\n\".join(code_files[:80])  # limita pra não explodir tokens\n",
        "\n",
        "system_prompt = \"Você é um arquiteto de software sênior.\"\n",
        "user_prompt = f\"\"\"\n",
        "Estou analisando a arquitetura de software do projeto open source Crawl4AI (https://github.com/unclecode/crawl4ai).\n",
        "\n",
        "Abaixo está a lista de arquivos Python principais dentro do pacote `crawl4ai`:\n",
        "\n",
        "{file_list_preview}\n",
        "\n",
        "1. Identifique as possíveis CAMADAS ou MÓDULOS (por exemplo: domínio, infraestrutura, interface, API, CLI, etc.).\n",
        "2. Aponte quais módulos parecem ser o \"core\" da biblioteca.\n",
        "3. Sugira qual estilo de arquitetura melhor descreve o projeto (por exemplo: arquitetura em camadas, modular, hexagonal, etc.).\n",
        "4. Traga hipóteses, deixando claro que são hipóteses baseadas na estrutura de arquivos.\n",
        "\n",
        "Responda em português, com tópicos bem organizados.\n",
        "\"\"\"\n",
        "\n",
        "macro_view = qwen_chat(system_prompt, user_prompt, max_tokens=900)\n",
        "print(macro_view)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8Ok1sWgZLH6",
        "outputId": "f5d1e356-100c-4725-dbc6-998ee9c46b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 1. Possíveis Camadas ou Módulos\n",
            "\n",
            "#### Domínio\n",
            "- **crawl4ai/crawlers/**: Contém classes e estratégias específicas para diferentes tipos de crawlers (como Google Search e Amazon Product). Essas camadas definem os comportamentos e regras de negócio relacionados à coleta de dados.\n",
            "- **crawl4ai/models.py**: Define modelos de dados que podem ser usados pela aplicação.\n",
            "- **crawl4ai/types.py**: Define tipos de dados personalizados que podem ser utilizados ao longo do projeto.\n",
            "\n",
            "#### Infraestrutura\n",
            "- **crawl4ai/async_database.py**: Gerencia a persistência assíncrona dos dados.\n",
            "- **crawl4ai/ssl_certificate.py**: Lida com certificados SSL.\n",
            "- **crawl4ai/driver_manager.py**: Gerencia drivers de navegador.\n",
            "- **crawl4ai/docker_client.py**: Interage com clientes Docker.\n",
            "- **crawl4ai/legacy/**: Contém versões legacy das funcionalidades principais, indicando uma evolução gradual da arquitetura.\n",
            "\n",
            "#### Interface\n",
            "- **crawl4ai/cli.py**: Fornece uma interface de linha de comando (CLI) para interagir com a aplicação.\n",
            "- **crawl4ai/html2text/**: Converte HTML em texto, sugerindo uma camada de apresentação ou utilitária.\n",
            "\n",
            "#### API\n",
            "- **crawl4ai/api/**: Se existisse, provavelmente conteria endpoints RESTful para interagir com o sistema.\n",
            "\n",
            "#### Processadores\n",
            "- **crawl4ai/processors/pdf/**: Contém processadores para lidar com PDFs, indicando uma camada específica para manipulação de formatos de arquivo.\n",
            "\n",
            "### 2. Módulos Parecem Ser o \"Core\" da Biblioteca\n",
            "\n",
            "Os módulos que parecem ser o \"core\" da biblioteca incluem:\n",
            "- **crawl4ai/crawlers/**: Este pacote contém as implementações específicas dos crawlers, que são essenciais para a coleta de dados.\n",
            "- **crawl4ai/async_crawler_strategy.py**: Define a estratégia de crawling assíncrona, que é fundamental para o funcionamento da biblioteca.\n",
            "- **crawl4ai/async_url_seeder.py**: Gerencia a sementeção de URLs para o crawler assíncrono.\n",
            "- **crawl4ai/async_configs.py**: Configurações assíncronas importantes para o funcionamento do crawler.\n",
            "- **crawl4ai/async_database.py**: Gerencia a persistência assíncrona dos dados coletados.\n",
            "- **crawl4ai/adaptive_crawler.py**: Implementa uma estratégia de crawling adaptativa, que pode ser considerada parte central da lógica de negócios.\n",
            "\n",
            "### 3. Estilo de Arquitetura Melhor Descreve o Projeto\n",
            "\n",
            "Baseado na estrutura de arquivos fornecida, o estilo de arquitetura que melhor descreve o projeto é a **Arquitetura Em Camadas** (Layered Architecture).\n",
            "\n",
            "Esta abordagem divide o código em camadas distintas, cada uma responsável por uma função específica:\n",
            "- **Apresentação (Interface)**: `cli.py`, `html2text/cli.py`.\n",
            "- **Negócio (Domínio)**: `crawlers/`, `models.py`, `types.py`, `adaptive_crawler.py`.\n",
            "- **Infraestrutura**: `browser_manager.py`, `async_database.py`, `ssl_certificate.py`, `docker_client.py`.\n",
            "\n",
            "A separação clara dessas camadas facilita a manutenção, escalabilidade e testabilidade do código. Além disso, a presença de camadas como `legacy/` sugere uma evolução gradual da arquitetura, onde novas camadas podem ser adicionadas sem afetar as camadas existentes.\n",
            "\n",
            "### 4. Hipóteses\n",
            "\n",
            "1. **Evolução Gradual**: A presença de uma pasta `legacy/` indica que o projeto está passando por uma evolução gradual. Novas funcionalidades estão sendo desenvolvidas e substituindo partes mais antigas do código.\n",
            "   \n",
            "2. **Crawler Adaptativo**: O arquivo `adaptive_crawler.py` sugere que o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib, textwrap\n",
        "\n",
        "def analisar_arquivo_arquitetura(path):\n",
        "    caminho = pathlib.Path(path)\n",
        "    codigo = caminho.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    # corta se estiver gigante (pra não explodir o contexto)\n",
        "    max_chars = 6000\n",
        "    if len(codigo) > max_chars:\n",
        "        codigo = codigo[:max_chars]\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "Arquivo: {path}\n",
        "\n",
        "Código:\n",
        "```python\n",
        "{codigo}\n",
        "```\n",
        "\n",
        "Com base na minha experiência como arquiteto de software, analise o código acima do ponto de vista de arquitetura de software:\n",
        "- Qual o papel desse arquivo no projeto (sua responsabilidade primária)?\n",
        "- A qual camada ou módulo ele pertence (domínio, infraestrutura, interface, etc.)? Explique.\n",
        "- Quais padrões de projeto podem ser observados (se houver)?\n",
        "- Há dependências visíveis com outros módulos ou bibliotecas?\n",
        "- Pontos fortes e possíveis melhorias (arquiteturalmente falando).\n",
        "\"\"\"\n",
        "    return qwen_chat(\n",
        "        \"Você é um arquiteto de software experiente em projetos Python.\",\n",
        "        user_prompt,\n",
        "        max_tokens=900\n",
        "    )"
      ],
      "metadata": {
        "id": "MkCPoVUdZMbd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib, textwrap\n",
        "\n",
        "def analisar_arquivo_arquitetura(path):\n",
        "    caminho = pathlib.Path(path)\n",
        "    codigo = caminho.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    # corta se estiver gigante (pra não explodir o contexto)\n",
        "    max_chars = 6000\n",
        "    if len(codigo) > max_chars:\n",
        "        codigo = codigo[:max_chars]\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "Arquivo: {path}\n",
        "\n",
        "Código:\n",
        "```python\n",
        "{codigo}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "mIN-HTAAZNdr",
        "outputId": "e1bb51bc-50f2-4568-9aa2-d6205fdbfad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-23133020.py, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-23133020.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    user_prompt = f\"\"\"\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_1",
        "outputId": "541a8d53-4a82-44c3-f940-21bbcbff54d8"
      },
      "source": [
        "resultados_micro = {}\n",
        "for f in code_files:\n",
        "    print(f\"Analisando {f}...\")\n",
        "    try:\n",
        "        resultados_micro[f] = analisar_arquivo_arquitetura(f)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao analisar {f}: {e}\")\n",
        "\n",
        "len(resultados_micro)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisando crawl4ai/user_agent_generator.py...\n",
            "Analisando crawl4ai/cli.py...\n",
            "Analisando crawl4ai/table_extraction.py...\n",
            "Analisando crawl4ai/browser_manager.py...\n",
            "Analisando crawl4ai/async_crawler_strategy.py...\n",
            "Analisando crawl4ai/async_url_seeder.py...\n",
            "Analisando crawl4ai/async_configs.py...\n",
            "Analisando crawl4ai/utils.py...\n",
            "Analisando crawl4ai/model_loader.py...\n",
            "Analisando crawl4ai/async_database.py...\n",
            "Analisando crawl4ai/proxy_strategy.py...\n",
            "Analisando crawl4ai/adaptive_crawler.py...\n",
            "Analisando crawl4ai/adaptive_crawler copy.py...\n",
            "Analisando crawl4ai/hub.py...\n",
            "Analisando crawl4ai/models.py...\n",
            "Analisando crawl4ai/types.py...\n",
            "Analisando crawl4ai/install.py...\n",
            "Analisando crawl4ai/__init__.py...\n",
            "Analisando crawl4ai/cache_context.py...\n",
            "Analisando crawl4ai/content_scraping_strategy.py...\n",
            "Analisando crawl4ai/prompts.py...\n",
            "Analisando crawl4ai/migrations.py...\n",
            "Analisando crawl4ai/async_dispatcher.py...\n",
            "Analisando crawl4ai/browser_profiler.py...\n",
            "Analisando crawl4ai/browser_adapter.py...\n",
            "Analisando crawl4ai/content_filter_strategy.py...\n",
            "Analisando crawl4ai/async_logger.py...\n",
            "Analisando crawl4ai/async_webcrawler.py...\n",
            "Analisando crawl4ai/link_preview.py...\n",
            "Analisando crawl4ai/docker_client.py...\n",
            "Analisando crawl4ai/async_crawler_strategy.back.py...\n",
            "Analisando crawl4ai/extraction_strategy.py...\n",
            "Analisando crawl4ai/markdown_generation_strategy.py...\n",
            "Analisando crawl4ai/config.py...\n",
            "Analisando crawl4ai/__version__.py...\n",
            "Analisando crawl4ai/ssl_certificate.py...\n",
            "Analisando crawl4ai/chunking_strategy.py...\n",
            "Analisando crawl4ai/js_snippet/__init__.py...\n",
            "Analisando crawl4ai/crawlers/__init__.py...\n",
            "Analisando crawl4ai/crawlers/google_search/__init__.py...\n",
            "Analisando crawl4ai/crawlers/google_search/crawler.py...\n",
            "Analisando crawl4ai/crawlers/amazon_product/__init__.py...\n",
            "Analisando crawl4ai/crawlers/amazon_product/crawler.py...\n",
            "Analisando crawl4ai/script/c4a_result.py...\n",
            "Analisando crawl4ai/script/__init__.py...\n",
            "Analisando crawl4ai/script/c4a_compile.py...\n",
            "Analisando crawl4ai/script/c4ai_script.py...\n",
            "Analisando crawl4ai/deep_crawling/dfs_strategy.py...\n",
            "Analisando crawl4ai/deep_crawling/base_strategy.py...\n",
            "Analisando crawl4ai/deep_crawling/filters.py...\n",
            "Analisando crawl4ai/deep_crawling/bff_strategy.py...\n",
            "Analisando crawl4ai/deep_crawling/crazy.py...\n",
            "Analisando crawl4ai/deep_crawling/__init__.py...\n",
            "Analisando crawl4ai/deep_crawling/bfs_strategy.py...\n",
            "Analisando crawl4ai/deep_crawling/scorers.py...\n",
            "Analisando crawl4ai/components/crawler_monitor.py...\n",
            "Analisando crawl4ai/legacy/cli.py...\n",
            "Analisando crawl4ai/legacy/llmtxt.py...\n",
            "Analisando crawl4ai/legacy/database.py...\n",
            "Analisando crawl4ai/legacy/version_manager.py...\n",
            "Analisando crawl4ai/legacy/crawler_strategy.py...\n",
            "Analisando crawl4ai/legacy/__init__.py...\n",
            "Analisando crawl4ai/legacy/web_crawler.py...\n",
            "Analisando crawl4ai/legacy/docs_manager.py...\n",
            "Analisando crawl4ai/html2text/cli.py...\n",
            "Analisando crawl4ai/html2text/utils.py...\n",
            "Analisando crawl4ai/html2text/__init__.py...\n",
            "Analisando crawl4ai/html2text/__main__.py...\n",
            "Analisando crawl4ai/html2text/_typing.py...\n",
            "Analisando crawl4ai/html2text/config.py...\n",
            "Analisando crawl4ai/html2text/elements.py...\n",
            "Analisando crawl4ai/processors/pdf/processor.py...\n",
            "Analisando crawl4ai/processors/pdf/utils.py...\n",
            "Analisando crawl4ai/processors/pdf/__init__.py...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/analise_micro_crawl4ai.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultados_micro, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "Rt0ng6S_ZOBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Junta algumas análises micro em um texto só\n",
        "micro_resumo = \"\"\n",
        "for path, txt in list(resultados_micro.items())[:8]:\n",
        "    micro_resumo += f\"\\n### Arquivo: {path}\\n{txt}\\n\"\n",
        "\n",
        "system_prompt = \"Você é um arquiteto de software preparando um relatório técnico para a faculdade.\"\n",
        "user_prompt = f\"\"\"\n",
        "Vou te passar:\n",
        "\n",
        "1) Uma visão macro da estrutura do projeto Crawl4AI.\n",
        "2) Várias análises micro de arquivos individuais.\n",
        "\n",
        "Com base nisso, produza um RELATÓRIO DE ARQUITETURA em português contendo:\n",
        "\n",
        "- Visão geral do projeto (o que ele faz, contexto geral).\n",
        "- Arquitetura de alto nível (estilo: camadas, modular, etc.).\n",
        "- Principais componentes/módulos e responsabilidades.\n",
        "- Como acontece a integração com navegador, rede, LLMs e Docker/API.\n",
        "- Padrões de projeto importantes detectados (por ex: pipeline, hooks, factories, etc.).\n",
        "- Pontos fortes da arquitetura.\n",
        "- Riscos ou possíveis melhorias.\n",
        "\n",
        "### Visão Macro\n",
        "{macro_view}\n",
        "\n",
        "### Visão Micro (por arquivo)\n",
        "{micro_resumo}\n",
        "\"\"\"\n",
        "\n",
        "relatorio_final = qwen_chat(system_prompt, user_prompt, max_tokens=1500)\n",
        "print(relatorio_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28CVQq5tZPlg",
        "outputId": "5b289027-5719-4489-811d-26d9473d5ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Relatório Técnico de Arquitetura do Projeto Crawl4AI\n",
            "\n",
            "## Visão Geral do Projeto\n",
            "\n",
            "O projeto Crawl4AI é uma biblioteca de robôs web assíncronos projetada para coletar dados de forma eficiente e escalável. Ele suporta a extração de tabelas, markdown, conteúdo de texto, e utiliza estratégias adaptativas para lidar com diferentes tipos de sites e conteúdos. O objetivo principal é facilitar a coleta de dados de forma legal e ética, permitindo aos usuários personalizar e expandir suas funcionalidades através de plugins e hooks.\n",
            "\n",
            "## Arquitetura de Alto Nível\n",
            "\n",
            "O estilo de arquitetura do projeto Crawl4AI é **Arquitetura Em Camadas** (Layered Architecture). Esta abordagem divide o código em camadas distintas, cada uma responsável por uma função específica:\n",
            "\n",
            "1. **Apresentação (Interface)**: Responsável pela interação direta com o usuário, incluindo a CLI (`crawl4ai/cli.py`) e a conversão de HTML para texto (`crawl4ai/html2text/cli.py`).\n",
            "2. **Negócio (Domínio)**: Contém as implementações específicas dos crawlers (`crawl4ai/crawlers/`), as estratégias de extração (`crawl4ai/extraction_strategy/`), e as lógicas de negócio principais.\n",
            "3. **Infraestrutura**: Gerencia a persistência assíncrona dos dados (`crawl4ai/async_database.py`), a comunicação com navegadores (`crawl4ai/browser_manager.py`), a manipulação de certificados SSL (`crawl4ai/ssl_certificate.py`), e a integração com LLMs (`crawl4ai/litellm_integration.py`).\n",
            "4. **Processadores**: Contém processadores específicos para lidar com formatos de arquivo, como PDFs (`crawl4ai/processors/pdf/`).\n",
            "\n",
            "## Principais Componentes/Módulos e Responsabilidades\n",
            "\n",
            "### Domínio\n",
            "\n",
            "- **crawl4ai/crawlers/**: Implementações específicas dos crawlers, como Google Search e Amazon Product.\n",
            "- **crawl4ai/extraction_strategy/**: Estratégias para extração de tabelas, markdown, conteúdo de texto, etc.\n",
            "- **crawl4ai/config/**: Classes de configuração para diferentes aspectos da aplicação, como estratégias de extração, markdown generation, scraping content, deep crawling, table extraction, cache context, proxy strategy e geolocation configuration.\n",
            "- **crawl4ai/types.py**: Definição de tipos de dados personalizados.\n",
            "\n",
            "### Infraestrutura\n",
            "\n",
            "- **crawl4ai/async_database.py**: Gerenciamento de persistência assíncrona dos dados.\n",
            "- **crawl4ai/ssl_certificate.py**: Lida com certificados SSL.\n",
            "- **crawl4ai/browser_manager.py**: Gerencia drivers de navegador.\n",
            "- **crawl4ai/docker_client.py**: Interage com clientes Docker.\n",
            "- **crawl4ai/legacy/**: Versões legacy das funcionalidades principais, indicando uma evolução gradual da arquitetura.\n",
            "\n",
            "### Interface\n",
            "\n",
            "- **crawl4ai/cli.py**: Interface de linha de comando para interagir com a aplicação.\n",
            "- **crawl4ai/html2text/**: Conversão de HTML em texto.\n",
            "\n",
            "### API\n",
            "\n",
            "- **crawl4ai/api/**: Endpoints RESTful para interagir com o sistema (caso exista).\n",
            "\n",
            "### Processadores\n",
            "\n",
            "- **crawl4ai/processors/pdf/**: Processadores para lidar com PDFs.\n",
            "\n",
            "## Integração com Navegador, Rede, LLMs e Docker/API\n",
            "\n",
            "### Navegador\n",
            "\n",
            "O projeto utiliza a biblioteca **Playwright** para controlar navegadores web de forma assíncrona. O gerenciamento de navegadores é realizado na camada de infraestrutura (`crawl4ai/browser_manager.py`), onde são iniciadas sessões persistentes através da API CDP (Chrome DevTools Protocol).\n",
            "\n",
            "### Rede\n",
            "\n",
            "A coleta de dados é realizada de forma assíncrona usando a biblioteca **httpx**, que suporta HTTP/2 e keep-alive. A sementeção de URLs é gerenciada pelo módulo `crawl4ai/async_url_seeder.py`, que utiliza o Common-Crawl e servidores HTTP para coletar URLs de forma eficiente.\n",
            "\n",
            "### LLMs\n",
            "\n",
            "Integração com modelos linguísticos grandes (LLMs) é realizada através do pacote **litellm** (`crawl4ai/litellm_integration.py`). Esta camada permite que o sistema utilize LLMs para gerar conteúdo, responder perguntas e realizar tarefas complexas.\n",
            "\n",
            "### Docker/API\n",
            "\n",
            "A camada de infraestrutura (`crawl4ai/docker_client.py`) interage com clientes Docker para containerizar e executar tarefas em ambientes isolados. Isso é particularmente útil para escalabilidade e segurança, permitindo que o sistema execute tarefas em containers separados.\n",
            "\n",
            "## Padrões de Projeto Importantes Detectados\n",
            "\n",
            "1. **Pipeline Pattern**: O projeto utiliza pipelines para processar dados coletados, onde cada etapa do pipeline realiza uma transformação específica.\n",
            "2. **Hooks Pattern**: Existem pontos de interrupção (hooks) em várias partes do código, permitindo que extensões ou plugins sejam inseridos sem modificar o código principal.\n",
            "3. **Factory Method Pattern**: O uso de fábricas para criar instâncias de classes de configuração e estratégias.\n",
            "4. **Decorator Pattern**: Decoradores como `@wraps` e `@lru_cache` são utilizados para manter a integridade dos metadados das funções e implementar cacheamento de resultados.\n",
            "5. **Strategy Pattern**: Estratégias específicas para diferentes tipos de coleta de dados, como crawlers assíncronos (`crawl4ai/async_crawler_strategy.py`), extração de tabelas (`crawl4ai/table_extraction.py`), etc.\n",
            "\n",
            "## Pontos Fortes da Arquitetura\n",
            "\n",
            "1. **Separação de Camadas**: A divisão clara em camadas facilita a manutenção, escalabilidade e testabilidade do código.\n",
            "2. **Modularidade**: O uso de padrões de projeto como Strategy e Factory Method permite a adição de novas funcionalidades sem afetar o código existente.\n",
            "3. **Evolução Gradual**: A presença de uma pasta `legacy/` indica uma evolução gradual da arquitetura, facilitando a migração de funcionalidades antigas para novas.\n",
            "4. **Concorrência Assíncrona**: O uso de asyncio e httpx para operações assíncronas melhora significativamente a performance do sistema.\n",
            "5. **Logging Assíncrono**: O uso de AsyncLogger para logging assíncrono garante que mensagens de log sejam registradas de forma eficiente.\n",
            "\n",
            "## Riscos ou Possíveis Melhorias\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}