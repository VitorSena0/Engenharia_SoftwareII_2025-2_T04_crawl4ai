# Tutorial ‚Äî An√°lise Arquitetural com Qwen/Qwen2.5-Coder-7B-Instruct  
### Notebook: `Analise_QWEN2.5-Coder-7B_Crawl4AI.ipynb`

Este tutorial descreve, passo a passo, como foi implementado o notebook que realiza a an√°lise arquitetural do reposit√≥rio `crawl4ai` utilizando o modelo **Qwen/Qwen2.5-Coder-7B-Instruct** via API do Hugging Face.

A ideia geral √©:

1. Configurar o ambiente no Google Colab;  
2. Conectar-se ao modelo Qwen 7B via API;  
3. Clonar o reposit√≥rio `crawl4ai`;  
4. Listar os arquivos `.py` do projeto;  
5. Fazer uma **an√°lise macro** (vis√£o geral da arquitetura);  
6. Fazer uma **an√°lise micro** (arquivo por arquivo);  
7. Gerar um **relat√≥rio t√©cnico de arquitetura** com base nas respostas do modelo.

---

## üß© 1. Infraestrutura Utilizada

A implementa√ß√£o foi feita no **Google Colab**, com:

- Conta Google (para acessar o Colab);  
- Conex√£o com internet (para acessar GitHub e Hugging Face);  
- **Sem necessidade de GPU**, pois o modelo √© acessado via API;  
- Um **token do Hugging Face** com permiss√£o de *Inference*.

> **Como implementar na pr√°tica:**  
> 1. Acesse: https://colab.research.google.com/  
> 2. Clique em **"Novo notebook"**.  
> 3. Use as c√©lulas exatamente na ordem apresentada neste tutorial.

---

## üõ†Ô∏è 2. Execu√ß√£o Passo a Passo

Cada subse√ß√£o abaixo representa uma c√©lula do notebook (ou um pequeno grupo de c√©lulas).

---

### üìå 2.1 ‚Äî Instalar Depend√™ncias

```python
!pip install -q huggingface_hub
```

**O que isso faz:**  
Instala a biblioteca `huggingface_hub`, que √© usada para se conectar √† API de infer√™ncia do Hugging Face.

**Como implementar no Colab:**  
- Crie uma nova c√©lula no topo do notebook;  
- Cole o comando acima;  
- Execute a c√©lula (Ctrl+Enter ou clicando no bot√£o de "play").

---

### üìå 2.2 ‚Äî Configurar Token e Cliente Hugging Face

```python
from huggingface_hub import InferenceClient
import os

HF_TOKEN = "hf_SEU_TOKEN_AQUI"

os.environ["HF_TOKEN"] = HF_TOKEN

client = InferenceClient(
    "Qwen/Qwen2.5-Coder-7B-Instruct",
    token=HF_TOKEN,
)
print("Token carregado (prefixo):", HF_TOKEN[:10])
```

**O que isso faz:**

- Importa a classe `InferenceClient` (para mandar requisi√ß√µes ao modelo);  
- Define a vari√°vel `HF_TOKEN` com seu token pessoal;  
- Opcionalmente, coloca o token numa vari√°vel de ambiente (`os.environ["HF_TOKEN"]`);  
- Cria o `client`, que √© o objeto usado para chamar o modelo Qwen 7B;  
- Imprime s√≥ o prefixo do token para confirmar que foi carregado.

**Como implementar:**

1. No Hugging Face, v√° em **Settings ‚Üí Access Tokens ‚Üí New Token** e crie um token com permiss√£o de *Inference*.  
2. Copie o token (come√ßa com `hf_...`).  
3. Substitua `hf_SEU_TOKEN_AQUI` pelo seu token (SEM commit√°-lo no GitHub!).  
4. Execute essa c√©lula no Colab.

---

### üìå 2.3 ‚Äî Fun√ß√£o utilit√°ria para conversar com o modelo

```python
def qwen_chat(system_prompt, user_prompt, max_tokens=1024, temperature=0.2):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user",   "content": user_prompt},
    ]
    try:
        resp = client.chat.completions.create(
            model="Qwen/Qwen2.5-Coder-7B-Instruct",
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return resp.choices[0].message.content
    except Exception as e:
        print("‚ùå Erro chamando o modelo:", e)
        raise
```

**O que isso faz:**

- Cria uma fun√ß√£o que facilita o uso do modelo;  
- Recebe:
  - um `system_prompt` (como o modelo deve se comportar, ex.: ‚ÄúVoc√™ √© um arquiteto de software s√™nior‚Äù);  
  - um `user_prompt` (a tarefa/quest√£o espec√≠fica);  
- Monta a lista de mensagens no formato esperado pela API;  
- Chama a API `client.chat.completions.create(...)`;  
- Retorna apenas o texto da resposta (`message.content`).

**Como implementar:**

- Crie uma nova c√©lula logo depois da configura√ß√£o do cliente HF;  
- Cole esse c√≥digo;  
- Execute a c√©lula.  
Depois disso, sempre que voc√™ quiser falar com o modelo, basta chamar `qwen_chat(...)`.

---

### üìå 2.4 ‚Äî Clonar o reposit√≥rio `crawl4ai`

```python
import os, shutil

REPO_URL = "https://github.com/unclecode/crawl4ai.git"
REPO_DIR = "/content/crawl4ai"

os.chdir('/content')

if os.path.exists(REPO_DIR):
    shutil.rmtree(REPO_DIR)

!git clone {REPO_URL} {REPO_DIR}
os.chdir(REPO_DIR)
```

**O que isso faz:**

- Define a URL do reposit√≥rio (`REPO_URL`) e o diret√≥rio de destino (`REPO_DIR`);  
- Garante que estamos em `/content` (diret√≥rio padr√£o do Colab);  
- Se j√° existir uma pasta `crawl4ai`, apaga ela (`shutil.rmtree`);  
- Usa o comando `git clone` para baixar o reposit√≥rio;  
- Entra na pasta do projeto com `os.chdir(REPO_DIR)`.

**Como implementar:**

- Crie uma nova c√©lula ap√≥s a fun√ß√£o `qwen_chat`;  
- Cole esse c√≥digo;  
- Execute.  
Ao final, voc√™ ter√° o c√≥digo do `crawl4ai` dispon√≠vel em `/content/crawl4ai`.

---

### üìå 2.5 ‚Äî Mapear arquivos Python

```python
import os

code_files = []

for root, dirs, files in os.walk("crawl4ai"):
    if "__pycache__" in root:
        continue

    for f in files:
        if f.endswith(".py"):
            code_files.append(os.path.join(root, f))

len(code_files), code_files[:15]
```

**O que isso faz:**

- Usa `os.walk` para percorrer todos os diret√≥rios dentro de `crawl4ai`;  
- Ignora pastas de cache (`__pycache__`);  
- Para cada arquivo que termina com `.py`, adiciona o caminho completo em `code_files`;  
- Ao final, mostra quantos arquivos foram encontrados e uma amostra dos primeiros 15.

**Como implementar:**

- Crie uma nova c√©lula;  
- Cole o c√≥digo;  
- Execute.  
Guarde a vari√°vel `code_files`, pois ela ser√° usada nas an√°lises macro e micro.

---

### üìå 2.6 ‚Äî An√°lise Macro da Arquitetura

```python
file_list_preview = "\n".join(code_files[:80])

system_prompt = "Voc√™ √© um arquiteto de software s√™nior."
user_prompt = f"""
Estou analisando a arquitetura de software do projeto Crawl4AI (https://github.com/unclecode/crawl4ai).

Abaixo est√° a lista de arquivos Python principais dentro do pacote `crawl4ai`:

{file_list_preview}

1. Identifique as poss√≠veis CAMADAS ou M√ìDULOS (por exemplo: dom√≠nio, infraestrutura, interface, API, CLI, etc.).
2. Aponte quais m√≥dulos parecem ser o "core" da biblioteca.
3. Sugira qual estilo de arquitetura melhor descreve o projeto (por exemplo: arquitetura em camadas, modular, hexagonal, etc.).
4. Traga hip√≥teses, deixando claro que s√£o hip√≥teses baseadas na estrutura de arquivos.

Responda em portugu√™s, com t√≥picos bem organizados.
"""

macro_view = qwen_chat(system_prompt, user_prompt, max_tokens=900)
print(macro_view)
```

**O que isso faz:**

- Junta at√© 80 caminhos de arquivos em uma string (`file_list_preview`);  
- Define que o modelo deve agir como um ‚Äúarquiteto de software s√™nior‚Äù;  
- Passa a lista de arquivos e pede:
  - identifica√ß√£o de camadas;  
  - m√≥dulos core;  
  - estilo arquitetural (camadas, hexagonal, etc.);  
  - hip√≥teses arquiteturais;  
- Chama `qwen_chat(...)` e salva a resposta em `macro_view`;  
- Imprime o resultado.

**Como implementar:**

- Nova c√©lula;  
- Cole e execute o c√≥digo;  
- Use o texto impresso como **vis√£o macro** da arquitetura no seu relat√≥rio.

---

### üìå 2.7 ‚Äî Fun√ß√£o para an√°lise Micro por arquivo

```python
import pathlib, textwrap

def analisar_arquivo_arquitetura(path):
    caminho = pathlib.Path(path)
    codigo = caminho.read_text(encoding="utf-8", errors="ignore")

    max_chars = 6000
    if len(codigo) > max_chars:
        codigo = codigo[:max_chars]

    user_prompt = f"""
Arquivo: {path}

C√≥digo:
```python
{codigo}
```

Com base na minha experi√™ncia como arquiteto de software, analise o c√≥digo acima do ponto de vista de arquitetura de software:
- Qual o papel desse arquivo no projeto (sua responsabilidade prim√°ria)?
- A qual camada ou m√≥dulo ele pertence (dom√≠nio, infraestrutura, interface, etc.)? Explique.
- Quais padr√µes de projeto podem ser observados (se houver)?
- H√° depend√™ncias vis√≠veis com outros m√≥dulos ou bibliotecas?
- Pontos fortes e poss√≠veis melhorias (arquiteturalmente falando).
"""
    return qwen_chat(
        "Voc√™ √© um arquiteto de software experiente em projetos Python.",
        user_prompt,
        max_tokens=900
    )
```

**O que isso faz:**

- L√™ o conte√∫do do arquivo passado em `path`;  
- Limita o tamanho do c√≥digo para no m√°ximo 6000 caracteres (para n√£o estourar o contexto do modelo);  
- Monta um prompt pedindo uma an√°lise detalhada:
  - responsabilidade do arquivo;  
  - camada;  
  - padr√µes de projeto;  
  - depend√™ncias;  
  - pontos fortes e melhorias;  
- Chama `qwen_chat(...)` com um system prompt espec√≠fico;  
- Retorna o texto da an√°lise.

**Como implementar:**

- Crie uma nova c√©lula ap√≥s a an√°lise macro;  
- Cole esse c√≥digo;  
- Execute.  
Essa fun√ß√£o ser√° usada em um loop para analisar todos os arquivos.

---

### üìå 2.8 ‚Äî Aplicar an√°lise Micro em todos os arquivos

```python
resultados_micro = {}
for f in code_files:
    print(f"Analisando {f}...")
    try:
        resultados_micro[f] = analisar_arquivo_arquitetura(f)
    except Exception as e:
        print(f"‚ùå Erro ao analisar {f}: {e}")

len(resultados_micro)
```

**O que isso faz:**

- Cria um dicion√°rio `resultados_micro`;  
- Para cada arquivo em `code_files`:
  - imprime qual arquivo est√° sendo analisado;  
  - chama `analisar_arquivo_arquitetura(f)`;  
  - armazena o resultado textual com a chave igual ao caminho do arquivo;  
- Ao final, mostra quantos arquivos foram analisados com sucesso.

**Como implementar:**

- Crie uma nova c√©lula;  
- Cole esse c√≥digo;  
- Execute.  
Dependendo da quantidade de arquivos, pode demorar (cada arquivo faz uma chamada √† API).

---

### üìå 2.9 ‚Äî Salvar resultados da an√°lise Micro

```python
import json

with open("/content/analise_micro_crawl4ai.json", "w", encoding="utf-8") as f:
    json.dump(resultados_micro, f, ensure_ascii=False, indent=2)
```

**O que isso faz:**

- Salva o dicion√°rio `resultados_micro` em um arquivo JSON;  
- Cada entrada do JSON cont√©m:
  - o caminho do arquivo Python;  
  - a an√°lise arquitetural textual gerada pelo modelo para aquele arquivo.

**Como implementar:**

- Nova c√©lula;  
- Cole e execute;  
- Depois, voc√™ pode baixar o arquivo pelo pr√≥prio Colab (barra lateral de arquivos).

---

### üìå 2.10 ‚Äî Gerar Relat√≥rio Final de Arquitetura

```python
micro_resumo = ""
for path, txt in list(resultados_micro.items())[:8]:
    micro_resumo += f"\n### Arquivo: {path}\n{txt}\n"

system_prompt = "Voc√™ √© um arquiteto de software preparando um relat√≥rio t√©cnico para a faculdade."
user_prompt = f"""
Vou te passar:

1) Uma vis√£o macro da estrutura do projeto Crawl4AI.
2) V√°rias an√°lises micro de arquivos individuais.

Com base nisso, produza um RELAT√ìRIO DE ARQUITETURA em portugu√™s contendo:

- Vis√£o geral do projeto (o que ele faz, contexto geral).
- Arquitetura de alto n√≠vel (estilo: camadas, modular, etc.).
- Principais componentes/m√≥dulos e responsabilidades.
- Como acontece a integra√ß√£o com navegador, rede, LLMs e Docker/API.
- Padr√µes de projeto importantes detectados (por ex: pipeline, hooks, factories, etc.).
- Pontos fortes da arquitetura.
- Riscos ou poss√≠veis melhorias.

### Vis√£o Macro
{macro_view}

### Vis√£o Micro (por arquivo)
{micro_resumo}
"""

relatorio_final = qwen_chat(system_prompt, user_prompt, max_tokens=1500)
print(relatorio_final)
```

**O que isso faz:**

- Monta um pequeno resumo (`micro_resumo`) com algumas an√°lises micro (para n√£o estourar tokens);  
- Passa para o modelo:
  - o texto da vis√£o macro (`macro_view`);  
  - o resumo das an√°lises micro;  
- Pede explicitamente um **RELAT√ìRIO DE ARQUITETURA** com:
  - vis√£o geral do projeto;  
  - arquitetura de alto n√≠vel;  
  - principais m√≥dulos e responsabilidades;  
  - integra√ß√µes com navegador, rede, LLMs e Docker;  
  - padr√µes de projeto;  
  - pontos fortes e melhorias;  
- Imprime o relat√≥rio final em texto.

**Como implementar:**

- Nova c√©lula;  
- Cole o c√≥digo;  
- Execute;  
- Copie o texto de `relatorio_final` e salve num `.md` ou `.pdf` para entregar como artefato da disciplina.


---

## ‚ö†Ô∏è 3. Limita√ß√µes

- O processo depende de um token v√°lido do Hugging Face;  
- √â necess√°rio internet para acessar a API;  
- A an√°lise √© **est√°tica** (o c√≥digo n√£o √© executado, apenas lido e interpretado como texto);  
- Arquivos muito grandes s√£o truncados para caber no limite de contexto do modelo.

---

## üéØ 4. Conclus√£o

Este notebook mostra um pipeline completo de **engenharia reversa assistida por IA**, aplicando o modelo **Qwen/Qwen2.5-Coder-7B-Instruct** para extrair:

- camadas arquiteturais;  
- responsabilidades de m√≥dulos;  
- padr√µes de projeto;  
- pontos fortes e oportunidades de melhoria  

no projeto `crawl4ai`.

---
