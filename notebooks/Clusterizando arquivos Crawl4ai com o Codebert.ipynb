{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Instalar as bibliotecas necessárias\n!pip install transformers\n!pip install scikit-learn\n\n!pip install protobuf==3.20.3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:27:25.788494Z","iopub.execute_input":"2025-11-10T00:27:25.788705Z","iopub.status.idle":"2025-11-10T00:27:39.429092Z","shell.execute_reply.started":"2025-11-10T00:27:25.788670Z","shell.execute_reply":"2025-11-10T00:27:39.428412Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\nCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!rm -rf crawl4ai\n\n!git clone https://github.com/unclecode/crawl4ai.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:27:39.431173Z","iopub.execute_input":"2025-11-10T00:27:39.431410Z","iopub.status.idle":"2025-11-10T00:27:47.023080Z","shell.execute_reply.started":"2025-11-10T00:27:39.431388Z","shell.execute_reply":"2025-11-10T00:27:47.022140Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'crawl4ai'...\nremote: Enumerating objects: 10062, done.\u001b[K\nremote: Counting objects: 100% (1944/1944), done.\u001b[K\nremote: Compressing objects: 100% (501/501), done.\u001b[K\nremote: Total 10062 (delta 1681), reused 1485 (delta 1443), pack-reused 8118 (from 2)\u001b[K\nReceiving objects: 100% (10062/10062), 144.23 MiB | 27.82 MiB/s, done.\nResolving deltas: 100% (6422/6422), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ls -R crawl4ai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:27:47.024260Z","iopub.execute_input":"2025-11-10T00:27:47.024638Z","iopub.status.idle":"2025-11-10T00:27:47.156024Z","shell.execute_reply.started":"2025-11-10T00:27:47.024603Z","shell.execute_reply":"2025-11-10T00:27:47.155151Z"}},"outputs":[{"name":"stdout","text":"crawl4ai:\nCHANGELOG.md\t    LICENSE\t\t     ROADMAP.md\ncliff.toml\t    MANIFEST.in\t\t     setup.cfg\nCODE_OF_CONDUCT.md  MISSION.md\t\t     setup.py\nCONTRIBUTORS.md     mkdocs.yml\t\t     SPONSORS.md\ncrawl4ai\t    PROGRESSIVE_CRAWLING.md  test_llm_webhook_feature.py\ndeploy\t\t    prompts\t\t     tests\ndocker-compose.yml  pyproject.toml\t     test_webhook_implementation.py\nDockerfile\t    README-first.md\t     uv.lock\ndocs\t\t    README.md\nJOURNAL.md\t    requirements.txt\n\ncrawl4ai/crawl4ai:\n'adaptive_crawler copy.py'\t  extraction_strategy.py\n adaptive_crawler.py\t\t  html2text\n async_configs.py\t\t  hub.py\n async_crawler_strategy.back.py   __init__.py\n async_crawler_strategy.py\t  install.py\n async_database.py\t\t  js_snippet\n async_dispatcher.py\t\t  legacy\n async_logger.py\t\t  link_preview.py\n async_url_seeder.py\t\t  markdown_generation_strategy.py\n async_webcrawler.py\t\t  migrations.py\n browser_adapter.py\t\t  model_loader.py\n browser_manager.py\t\t  models.py\n browser_profiler.py\t\t  processors\n cache_context.py\t\t  prompts.py\n chunking_strategy.py\t\t  proxy_strategy.py\n cli.py\t\t\t\t  script\n components\t\t\t  ssl_certificate.py\n config.py\t\t\t  table_extraction.py\n content_filter_strategy.py\t  types.py\n content_scraping_strategy.py\t  user_agent_generator.py\n crawlers\t\t\t  utils.py\n deep_crawling\t\t\t  __version__.py\n docker_client.py\n\ncrawl4ai/crawl4ai/components:\ncrawler_monitor.py\n\ncrawl4ai/crawl4ai/crawlers:\namazon_product\tgoogle_search  __init__.py\n\ncrawl4ai/crawl4ai/crawlers/amazon_product:\ncrawler.py  __init__.py\n\ncrawl4ai/crawl4ai/crawlers/google_search:\ncrawler.py  __init__.py  script.js\n\ncrawl4ai/crawl4ai/deep_crawling:\nbase_strategy.py  bfs_strategy.py  dfs_strategy.py  __init__.py\nbff_strategy.py   crazy.py\t   filters.py\t    scorers.py\n\ncrawl4ai/crawl4ai/html2text:\ncli.py\tconfig.py  elements.py\t__init__.py  __main__.py  _typing.py  utils.py\n\ncrawl4ai/crawl4ai/js_snippet:\n__init__.py\t\tremove_overlay_elements.js\nnavigator_overrider.js\tupdate_image_dimensions.js\n\ncrawl4ai/crawl4ai/legacy:\ncli.py\t\t     database.py      __init__.py  version_manager.py\ncrawler_strategy.py  docs_manager.py  llmtxt.py    web_crawler.py\n\ncrawl4ai/crawl4ai/processors:\npdf\n\ncrawl4ai/crawl4ai/processors/pdf:\n__init__.py  processor.py  utils.py\n\ncrawl4ai/crawl4ai/script:\nc4a_compile.py\tc4ai_script.py\tc4a_result.py  __init__.py\n\ncrawl4ai/deploy:\ndocker\n\ncrawl4ai/deploy/docker:\napi.py\t\t      crawler_pool.py  requirements.txt  utils.py\nauth.py\t\t      hook_manager.py  schemas.py\t WEBHOOK_EXAMPLES.md\nc4ai-code-context.md  job.py\t       server.py\t webhook.py\nc4ai-doc-context.md   mcp_bridge.py    static\nconfig.yml\t      README.md        supervisord.conf\n\ncrawl4ai/deploy/docker/static:\nplayground\n\ncrawl4ai/deploy/docker/static/playground:\nindex.html\n\ncrawl4ai/docs:\napps\tblog\t  deprecated  md_v2\t       snippets\nassets\tcodebase  examples    releases_review  tutorials\n\ncrawl4ai/docs/apps:\niseeyou  linkdin\n\ncrawl4ai/docs/apps/iseeyou:\nllms-full.txt\n\ncrawl4ai/docs/apps/linkdin:\nc4ai_discover.py\t\t\t       samples\nc4ai_insights.py\t\t\t       schemas\nCrawl4ai_Linkedin_Data_Discovery_Part_1.ipynb  snippets\nCrawl4ai_Linkedin_Data_Discovery_Part_2.ipynb  templates\nREADME.md\n\ncrawl4ai/docs/apps/linkdin/samples:\ncompanies.jsonl  people.jsonl\n\ncrawl4ai/docs/apps/linkdin/schemas:\ncompany_card.json  people_card.json\n\ncrawl4ai/docs/apps/linkdin/snippets:\ncompany.html  people.html\n\ncrawl4ai/docs/apps/linkdin/templates:\nai.js  graph_view_template.html\n\ncrawl4ai/docs/assets:\npitch-dark.png\tpowered-by-dark.svg   powered-by-light.svg\npitch-dark.svg\tpowered-by-disco.svg  powered-by-night.svg\n\ncrawl4ai/docs/blog:\nrelease-v0.7.0.md  release-v0.7.3.md  release-v0.7.5.md\nrelease-v0.7.1.md  release-v0.7.4.md  release-v0.7.6.md\n\ncrawl4ai/docs/codebase:\nbrowser.md  cli.md\n\ncrawl4ai/docs/deprecated:\ndocker-deployment.md\n\ncrawl4ai/docs/examples:\nadaptive_crawling\namazon_product_extraction_direct_url.py\namazon_product_extraction_using_hooks.py\namazon_product_extraction_using_use_javascript.py\narun_vs_arun_many.py\nassets\nasync_webcrawler_multiple_urls_example.py\nbrowser_optimization_example.py\nbuiltin_browser_example.py\nc4a_script\nchainlit.md\ncli\ncrawlai_vs_firecrawl.py\ncrawler_monitor_example.py\ncrypto_analysis_example.py\ndeepcrawl_example.py\ndemo_multi_config_clean.py\ndispatcher_example.py\ndocker\ndocker_client_hooks_example.py\ndocker_config_obj.py\ndocker_example.py\ndocker_hooks_examples.py\ndocker_python_rest_api.py\ndocker_python_sdk.py\ndocker_webhook_example.py\nextraction_strategies_examples.py\nfull_page_screenshot_and_pdf_export.md\nhello_world.py\nhello_world_undetected.py\nhooks_example.py\nidentity_based_browsing.py\nlanguage_support_example.py\nlink_head_extraction_example.py\nllm_extraction_openai_pricing.py\nllm_markdown_generator.py\nllm_table_extraction_example.py\nmarkdown\nnetwork_console_capture_example.py\nproxy_rotation_demo.py\nquickstart_examples_set_1.py\nquickstart_examples_set_2.py\nquickstart.ipynb\nquickstart.py\nREADME_BUILTIN_BROWSER.md\nregex_extraction_quickstart.py\nresearch_assistant.py\nrest_call.py\nsample_ecommerce.html\nscraping_strategies_performance.py\nserp_api_project_11_feb.py\nsession_id_example.py\nsimple_anti_bot_examples.py\nssl_example.py\nstealth_mode_example.py\nstealth_mode_quick_start.py\nstealth_test_simple.py\nstorage_state_tutorial.md\nsummarize_page.py\ntable_extraction_example.py\ntutorial_dynamic_clicks.md\ntutorial_v0.5.py\nundetectability\nundetected_simple_demo.py\nurl_seeder\nuse_geo_location.py\nvirtual_scroll_example.py\nwebsite-to-api\n\ncrawl4ai/docs/examples/adaptive_crawling:\nadvanced_configuration.py  embedding_configuration.py\texport_import_kb.py\nbasic_usage.py\t\t   embedding_strategy.py\tllm_config_example.py\ncustom_strategies.py\t   embedding_vs_statistical.py\tREADME.md\n\ncrawl4ai/docs/examples/assets:\naudio.mp3\t\t   llm_extraction.png\nbasic.png\t\t   semantic_extraction_cosine.png\ncosine_extraction.png\t   semantic_extraction_llm.png\ncss_js.png\t\t   virtual_scroll_append_only.html\ncss_selector.png\t   virtual_scroll_instagram_grid.html\nexec_script.png\t\t   virtual_scroll_news_feed.html\ninstagram_grid_result.png  virtual_scroll_twitter_like.html\n\ncrawl4ai/docs/examples/c4a_script:\namazon_example\t\t\t c4a_script_hello_world.py\t github_search\napi_usage_examples.py\t\t demo_c4a_crawl4ai.py\t\t script_samples\nc4a_script_hello_world_error.py  generate_script_hello_world.py  tutorial\n\ncrawl4ai/docs/examples/c4a_script/amazon_example:\namazon_r2d2_search.py\t generated_product_schema.json\theader.html   README.md\nextracted_products.json  generated_search_script.js\tproduct.html\n\ncrawl4ai/docs/examples/c4a_script/github_search:\nextracted_repositories.json   generated_search_script.js  result.html\ngenerated_result_schema.json  github_search_crawler.py\t  search_form.html\n\ncrawl4ai/docs/examples/c4a_script/script_samples:\nadd_to_cart.c4a\t\t   load_more_content.c4a    responsive_actions.c4a\nadvanced_control_flow.c4a  login_flow.c4a\t    scroll_and_click.c4a\nconditional_login.c4a\t   multi_step_workflow.c4a  search_product.c4a\ndata_extraction.c4a\t   navigate_tabs.c4a\t    simple_form.c4a\nfill_contact.c4a\t   quick_login.c4a\t    smart_form_fill.c4a\n\ncrawl4ai/docs/examples/c4a_script/tutorial:\nassets\t\t  index.html  README.md\t\tscripts    test_blockly.html\nblockly-demo.c4a  playground  requirements.txt\tserver.py\n\ncrawl4ai/docs/examples/c4a_script/tutorial/assets:\napp.css\t\t    blockly-theme.css  DankMono-Bold.woff2     styles.css\napp.js\t\t    c4a-blocks.js      DankMono-Italic.woff2\nblockly-manager.js  c4a-generator.js   DankMono-Regular.woff2\n\ncrawl4ai/docs/examples/c4a_script/tutorial/playground:\napp.js\tindex.html  styles.css\n\ncrawl4ai/docs/examples/c4a_script/tutorial/scripts:\n01-basic-interaction.c4a  03-infinite-scroll.c4a  05-complex-workflow.c4a\n02-login-flow.c4a\t  04-multi-step-form.c4a\n\ncrawl4ai/docs/examples/cli:\nbrowser.yml  css_schema.json  extract.yml\ncrawler.yml  extract_css.yml  llm_schema.json\n\ncrawl4ai/docs/examples/docker:\ndemo_docker_api.py  demo_docker_polling.py\n\ncrawl4ai/docs/examples/markdown:\ncontent_source_example.py  content_source_short_example.py\n\ncrawl4ai/docs/examples/undetectability:\nundetected_basic_test.py  undetected_cloudflare_test.py\nundetected_bot_test.py\t  undetected_vs_regular_comparison.py\n\ncrawl4ai/docs/examples/url_seeder:\nbbc_sport_research_assistant.py     tutorial_url_seeder.md\nconvert_tutorial_to_colab.py\t    url_seeder_demo.py\nCrawl4AI_URL_Seeder_Tutorial.ipynb  url_seeder_quick_demo.py\n\ncrawl4ai/docs/examples/website-to-api:\napi_server.py  assets\t  requirements.txt  test_api.py     web_scraper_lib.py\napp.py\t       README.md  static\t    test_models.py\n\ncrawl4ai/docs/examples/website-to-api/assets:\ncrawl4ai_logo.jpg\n\ncrawl4ai/docs/examples/website-to-api/static:\nindex.html  script.js  styles.css\n\ncrawl4ai/docs/md_v2:\nadvanced  assets    complete-sdk-reference.md  img\t    overrides\napi\t  basic     core\t\t       index.md\napps\t  blog\t    extraction\t\t       marketplace\nask_ai\t  branding  favicon.ico\t\t       migration\n\ncrawl4ai/docs/md_v2/advanced:\nadaptive-strategies.md\tidentity-based-crawling.md  proxy-security.md\nadvanced-features.md\tlazy-loading.md\t\t    session-management.md\ncrawl-dispatcher.md\tmulti-url-crawling.md\t    ssl-certificate.md\nfile-downloading.md\tnetwork-console-capture.md  undetected-browser.md\nhooks-auth.md\t\tpdf-parsing.md\t\t    virtual-scroll.md\n\ncrawl4ai/docs/md_v2/api:\nadaptive-crawler.md  async-webcrawler.md      digest.md\narun_many.md\t     c4a-script-reference.md  parameters.md\narun.md\t\t     crawl-result.md\t      strategies.md\n\ncrawl4ai/docs/md_v2/apps:\nassets\tc4a-script  crawl4ai-assistant\tindex.md  llmtxt\n\ncrawl4ai/docs/md_v2/apps/assets:\nDankMono-Bold.woff2  DankMono-Italic.woff2  DankMono-Regular.woff2\n\ncrawl4ai/docs/md_v2/apps/c4a-script:\nassets\t\t  index.html  README.md\t\tscripts    test_blockly.html\nblockly-demo.c4a  playground  requirements.txt\tserver.py\n\ncrawl4ai/docs/md_v2/apps/c4a-script/assets:\napp.css\t\t    blockly-theme.css  DankMono-Bold.woff2     styles.css\napp.js\t\t    c4a-blocks.js      DankMono-Italic.woff2\nblockly-manager.js  c4a-generator.js   DankMono-Regular.woff2\n\ncrawl4ai/docs/md_v2/apps/c4a-script/playground:\napp.js\tindex.html  styles.css\n\ncrawl4ai/docs/md_v2/apps/c4a-script/scripts:\n01-basic-interaction.c4a  03-infinite-scroll.c4a  05-complex-workflow.c4a\n02-login-flow.c4a\t  04-multi-step-form.c4a\n\ncrawl4ai/docs/md_v2/apps/crawl4ai-assistant:\nassets\t       content\t\t\t      icons\t  manifest.json\nassistant.css  crawl4ai-assistant-v1.2.1.zip  index.html  popup\nbackground     crawl4ai-assistant-v1.3.0.zip  libs\t  README.md\n\ncrawl4ai/docs/md_v2/apps/crawl4ai-assistant/assets:\nDankMono-Bold.woff2  DankMono-Italic.woff2  DankMono-Regular.woff2\n\ncrawl4ai/docs/md_v2/apps/crawl4ai-assistant/background:\nservice-worker.js\n\ncrawl4ai/docs/md_v2/apps/crawl4ai-assistant/content:\nclick2crawl.js\t    markdownConverter.js     overlay.css\ncontentAnalyzer.js  markdownExtraction.js    scriptBuilder.js\ncontent.js\t    markdownPreviewModal.js  shared\n\ncrawl4ai/docs/md_v2/apps/crawl4ai-assistant/content/shared:\nutils.js\n\ncrawl4ai/docs/md_v2/apps/crawl4ai-assistant/icons:\nfavicon.ico  icon-128.png  icon-16.png\ticon-48.png\n\ncrawl4ai/docs/md_v2/apps/crawl4ai-assistant/libs:\nmarked.min.js\n\ncrawl4ai/docs/md_v2/apps/crawl4ai-assistant/popup:\nicons  popup.css  popup.html  popup.js\n\ncrawl4ai/docs/md_v2/apps/crawl4ai-assistant/popup/icons:\nfavicon.ico  icon-128.png  icon-16.png\ticon-48.png\n\ncrawl4ai/docs/md_v2/apps/llmtxt:\nbuild.md  index.html  llmtxt.css  llmtxt.js  why.md\n\ncrawl4ai/docs/md_v2/ask_ai:\nask-ai.css  ask-ai.js  index.html\n\ncrawl4ai/docs/md_v2/assets:\ncopy_code.js\t\t   github_stats.js    Monaco.woff\ncrawl4ai-skill.zip\t   gtag.js\t      page_actions.css\nDankMono-Bold.woff2\t   highlight.css      page_actions.js\nDankMono-Italic.woff2\t   highlight_init.js  selection_ask_ai.js\nDankMono-Regular.woff2\t   highlight.min.js   styles.css\ndmvendor.css\t\t   images\t      test\ndocs.zip\t\t   layout.css\t      toc.js\nfeedback-overrides.css\t   llm.txt\nfloating_ask_ai_button.js  mobile_menu.js\n\ncrawl4ai/docs/md_v2/assets/images:\ndispatcher.png\tlogo.png\n\ncrawl4ai/docs/md_v2/assets/llm.txt:\ndiagrams  txt\n\ncrawl4ai/docs/md_v2/assets/llm.txt/diagrams:\ncli.txt\t\t\t\t\t http_based_crawler_strategy.txt\nconfig_objects.txt\t\t\t installation.txt\ndeep_crawl_advanced_filters_scorers.txt  llms-diagram.txt\ndeep_crawling.txt\t\t\t multi_urls_crawling.txt\ndocker.txt\t\t\t\t simple_crawling.txt\nextraction-llm.txt\t\t\t url_seeder.txt\nextraction-no-llm.txt\n\ncrawl4ai/docs/md_v2/assets/llm.txt/txt:\ncli.txt\t\t\t\t\t http_based_crawler_strategy.txt\nconfig_objects.txt\t\t\t installation.txt\ndeep_crawl_advanced_filters_scorers.txt  llms-full.txt\ndeep_crawling.txt\t\t\t llms-full-v0.1.1.txt\ndocker.txt\t\t\t\t multi_urls_crawling.txt\nextraction-llm.txt\t\t\t simple_crawling.txt\nextraction-no-llm.txt\t\t\t url_seeder.txt\n\ncrawl4ai/docs/md_v2/assets/test:\ntoc.js\n\ncrawl4ai/docs/md_v2/basic:\ninstallation.md\n\ncrawl4ai/docs/md_v2/blog:\narticles  index.md  index.md.bak  releases\n\ncrawl4ai/docs/md_v2/blog/articles:\nadaptive-crawling-revolution.md  llm-context-revolution.md\ndockerize_hooks.md\t\t virtual-scroll-revolution.md\n\ncrawl4ai/docs/md_v2/blog/releases:\n0.4.0.md  0.4.2.md  0.6.0.md  0.7.1.md\t0.7.3.md  v0.4.3b1.md\n0.4.1.md  0.5.0.md  0.7.0.md  0.7.2.md\t0.7.6.md  v0.7.5.md\n\ncrawl4ai/docs/md_v2/branding:\nindex.md\n\ncrawl4ai/docs/md_v2/core:\nadaptive-crawling.md\t   deep-crawling.md\t markdown-generation.md\nask-ai.md\t\t   docker-deployment.md  page-interaction.md\nbrowser-crawler-config.md  examples.md\t\t quickstart.md\nc4a-script.md\t\t   fit-markdown.md\t simple-crawling.md\ncache-modes.md\t\t   installation.md\t table_extraction.md\ncli.md\t\t\t   link-media.md\t url-seeding.md\ncontent-selection.md\t   llmtxt.md\ncrawler-result.md\t   local-files.md\n\ncrawl4ai/docs/md_v2/extraction:\nchunking.md  clustring-strategies.md  llm-strategies.md  no-llm-strategies.md\n\ncrawl4ai/docs/md_v2/img:\nfavicon-32x32.png  favicon.ico\tfavicon-x-32x32.png\n\ncrawl4ai/docs/md_v2/marketplace:\nadmin\t\tapp-detail.html  backend   index.html\t    marketplace.js\napp-detail.css\tapp-detail.js\t frontend  marketplace.css  README.md\n\ncrawl4ai/docs/md_v2/marketplace/admin:\nadmin.css  admin.js  index.html\n\ncrawl4ai/docs/md_v2/marketplace/backend:\nconfig.py    dummy_data.py     schema.yaml  uploads\ndatabase.py  requirements.txt  server.py\n\ncrawl4ai/docs/md_v2/marketplace/backend/uploads:\n\ncrawl4ai/docs/md_v2/marketplace/frontend:\napp-detail.css\t app-detail.js\tmarketplace.css\napp-detail.html  index.html\tmarketplace.js\n\ncrawl4ai/docs/md_v2/migration:\ntable_extraction_v073.md  webscraping-strategy-migration.md\n\ncrawl4ai/docs/md_v2/overrides:\nmain.html\n\ncrawl4ai/docs/releases_review:\nCrawl4AI_v0.3.72_Release_Announcement.ipynb  v0_4_24_walkthrough.py\ncrawl4ai_v0_7_0_showcase.py\t\t     v0_4_3b2_features_demo.py\ndemo_v0.7.0.py\t\t\t\t     v0_7_0_features_demo.py\ndemo_v0.7.5.py\t\t\t\t     v0.7.5_docker_hooks_demo.py\ndemo_v0.7.6.py\t\t\t\t     v0.7.5_video_walkthrough.ipynb\nv0.3.74.overview.py\n\ncrawl4ai/docs/snippets:\ndeep_crawl\n\ncrawl4ai/docs/snippets/deep_crawl:\n1.intro.py  2.filters.py\n\ncrawl4ai/docs/tutorials:\ncoming_soon.md\n\ncrawl4ai/prompts:\nprompt_net_requests.md\n\ncrawl4ai/tests:\nadaptive\t       test_cli_docs.py\nasync\t\t       test_config_matching_only.py\nasync_assistant        test_config_selection.py\nbrowser\t\t       test_docker_api_with_llm_provider.py\ncheck_dependencies.py  test_docker.py\ncli\t\t       test_link_extractor.py\ndeep_crwaling\t       test_llm_simple_url.py\ndocker\t\t       test_llmtxt.py\ndocker_example.py      test_main.py\ngeneral\t\t       test_memory_macos.py\nhub\t\t       test_multi_config.py\n__init__.py\t       test_normalize_url.py\nloggers\t\t       test_preserve_https_for_internal_links.py\nmcp\t\t       test_scraping_strategy.py\nmemory\t\t       test_virtual_scroll.py\nprofiler\t       test_web_crawler.py\nproxy\t\t       test_webhook_feature.sh\nreleases\t       WEBHOOK_TEST_README.md\ntest_arun_many.py\n\ncrawl4ai/tests/adaptive:\ncompare_performance.py\t  test_embedding_performance.py\ntest_adaptive_crawler.py  test_embedding_strategy.py\ntest_confidence_debug.py  test_llm_embedding.py\n\ncrawl4ai/tests/async:\nsample_wikipedia.html\ntest_0.4.2_browser_manager.py\ntest_0.4.2_config_params.py\ntest_async_doanloader.py\ntest_basic_crawling.py\ntest_caching.py\ntest_chunking_and_extraction_strategies.py\ntest_content_extraction.py\ntest_content_filter_bm25.py\ntest_content_filter_prune.py\ntest_content_scraper_strategy.py\ntest_crawler_strategy.py\ntest_database_operations.py\ntest_dispatchers.py\ntest_edge_cases.py\ntest_error_handling.py\ntest_evaluation_scraping_methods_performance.configs.py\ntest_markdown_genertor.py\ntest_parameters_and_options.py\ntest_performance.py\ntest_screenshot.py\n\ncrawl4ai/tests/async_assistant:\ntest_extract_pipeline.py  test_extract_pipeline_v2.py\n\ncrawl4ai/tests/browser:\ndocker\t\t\t test_builtin_strategy.py   test_parallel_crawling.py\nmanager\t\t\t test_cdp_strategy.py\t    test_playwright_strategy.py\ntest_browser_manager.py  test_combined.py\t    test_profiles.py\ntest_builtin_browser.py  test_launch_standalone.py\n\ncrawl4ai/tests/browser/docker:\n__init__.py  test_docker_browser.py\n\ncrawl4ai/tests/browser/manager:\ndemo_browser_manager.py\n\ncrawl4ai/tests/cli:\ntest_cli.py\n\ncrawl4ai/tests/deep_crwaling:\ntest_filter.py\n\ncrawl4ai/tests/docker:\nsimple_api_test.py\t   test_hooks_client.py\t\ttest_serialization.py\ntest_config_object.py\t   test_hooks_comprehensive.py\ttest_server.py\ntest_dockerclient.py\t   test_hooks_utility.py\ttest_server_requests.py\ntest_docker.py\t\t   test_llm_params.py\t\ttest_server_token.py\ntest_filter_deep_crawl.py  test_rest_api_deep_crawl.py\n\ncrawl4ai/tests/general:\ngenerate_dummy_site.py\t\t\t       test_download_file.py\ntest_acyn_crawl_wuth_http_crawler_strategy.py  test_http_crawler_strategy.py\ntest_advanced_deep_crawl.py\t\t       test_llm_filter.py\ntest_async_crawler_strategy.py\t\t       test_max_scroll.py\ntest_async_markdown_generator.py\t       test_mhtml.py\ntest_async_url_seeder_bm25.py\t\t       test_network_console_capture.py\ntest_async_webcrawler.py\t\t       test_persistent_context.py\ntest_bff_scoring.py\t\t\t       test_robot_parser.py\ntest_cache_context.py\t\t\t       test_schema_builder.py\ntest_content_source_parameter.py\t       test_stream_dispatch.py\ntest_crawlers.py\t\t\t       test_stream.py\ntest_deep_crawl_filters.py\t\t       test_url_pattern.py\ntest_deep_crawl.py\t\t\t       tets_robot.py\ntest_deep_crawl_scorers.py\n\ncrawl4ai/tests/hub:\ntest_simple.py\n\ncrawl4ai/tests/loggers:\ntest_logger.py\n\ncrawl4ai/tests/mcp:\ntest_mcp_socket.py  test_mcp_sse.py\n\ncrawl4ai/tests/memory:\nbenchmark_report.py  run_benchmark.py\t\ttest_stress_api.py\ncap_test.py\t     test_crawler_monitor.py\ttest_stress_api_xs.py\nREADME.md\t     test_dispatcher_stress.py\ttest_stress_docker_api.py\nrequirements.txt     test_docker_config_gen.py\ttest_stress_sdk.py\n\ncrawl4ai/tests/profiler:\ntest_create_profile.py\ttest_keyboard_handle.py\n\ncrawl4ai/tests/proxy:\ntest_proxy_config.py  test_proxy_deprecation.py\n\ncrawl4ai/tests/releases:\ntest_release_0.6.4.py  test_release_0.7.0.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nimport os\nimport numpy as np\n\n# Definir o nome do modelo\nmodel_name = \"microsoft/codebert-base\"\n\nprint(\"Carregando tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(\"Carregando modelo...\")\nmodel = AutoModel.from_pretrained(model_name)\n\nmodel.eval()\n\nprint(\"Modelo e tokenizer carregados com sucesso!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:27:47.157287Z","iopub.execute_input":"2025-11-10T00:27:47.157619Z","iopub.status.idle":"2025-11-10T00:28:48.459140Z","shell.execute_reply.started":"2025-11-10T00:27:47.157584Z","shell.execute_reply":"2025-11-10T00:28:48.458381Z"}},"outputs":[{"name":"stdout","text":"Carregando tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4fde3d15f48456a894ce5e9c85d5412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a766074d564c8f8848d5cea4b4f814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6c57b1436584d318919b2858a85d6e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27d18fc61cf24350a6882a4f894b6826"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6456cd97209049a88ba733a4e6078f17"}},"metadata":{}},{"name":"stdout","text":"Carregando modelo...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-11-10 00:28:15.222527: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762734495.629059      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762734495.767754      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a520d9d092c2459abdbf0ff7beb55e07"}},"metadata":{}},{"name":"stdout","text":"Modelo e tokenizer carregados com sucesso!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def get_code_embedding(file_path):\n    \"\"\"\n    Lê um arquivo de código-fonte e retorna seu embedding\n    usando o token [CLS] do CodeBERT.\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            code = f.read()\n        \n        if not code.strip():\n            return None\n            \n        # Tokenizar o código\n        # Usamos truncation=True para garantir que arquivos longos\n        # sejam cortados no tamanho máximo do modelo (512 tokens)\n        inputs = tokenizer(\n            code, \n            return_tensors=\"pt\", \n            truncation=True, \n            max_length=512, \n            padding=\"max_length\"\n        )\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n        \n        cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n        \n        return np.squeeze(cls_embedding)\n        \n    except Exception as e:\n        print(f\"Erro ao processar {file_path}: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:28:48.459931Z","iopub.execute_input":"2025-11-10T00:28:48.460531Z","iopub.status.idle":"2025-11-10T00:28:48.466288Z","shell.execute_reply.started":"2025-11-10T00:28:48.460510Z","shell.execute_reply":"2025-11-10T00:28:48.465423Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Caminho base para o código-fonte do projeto clonado\nrepo_path = 'crawl4ai/crawl4ai'\n\nfile_embeddings = {}\nfile_paths = []\n\nprint(f\"Iniciando análise de embeddings para {repo_path}...\")\n\n# Navegar por todos os diretórios e subdiretórios\nfor root, dirs, files in os.walk(repo_path):\n    for file in files:\n        # Processar apenas arquivos Python\n        if file.endswith('.py'):\n            file_path = os.path.join(root, file)\n            \n            embedding = get_code_embedding(file_path)\n            \n            # Adicionar apenas se o embedding foi gerado com sucesso\n            if embedding is not None:\n                print(f\"  - Processado: {file_path}\")\n                file_paths.append(file_path)\n                file_embeddings[file_path] = embedding\n\nprint(f\"\\nAnálise concluída. {len(file_embeddings)} arquivos .py processados e vetorizados.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:28:48.467260Z","iopub.execute_input":"2025-11-10T00:28:48.467604Z","iopub.status.idle":"2025-11-10T00:29:43.451622Z","shell.execute_reply.started":"2025-11-10T00:28:48.467583Z","shell.execute_reply":"2025-11-10T00:29:43.450690Z"}},"outputs":[{"name":"stdout","text":"Iniciando análise de embeddings para crawl4ai/crawl4ai...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fbc4634cd684cb28420a5139342bfd3"}},"metadata":{}},{"name":"stdout","text":"  - Processado: crawl4ai/crawl4ai/types.py\n  - Processado: crawl4ai/crawl4ai/browser_adapter.py\n  - Processado: crawl4ai/crawl4ai/async_logger.py\n  - Processado: crawl4ai/crawl4ai/content_scraping_strategy.py\n  - Processado: crawl4ai/crawl4ai/config.py\n  - Processado: crawl4ai/crawl4ai/proxy_strategy.py\n  - Processado: crawl4ai/crawl4ai/async_webcrawler.py\n  - Processado: crawl4ai/crawl4ai/browser_profiler.py\n  - Processado: crawl4ai/crawl4ai/__version__.py\n  - Processado: crawl4ai/crawl4ai/docker_client.py\n  - Processado: crawl4ai/crawl4ai/async_crawler_strategy.back.py\n  - Processado: crawl4ai/crawl4ai/model_loader.py\n  - Processado: crawl4ai/crawl4ai/chunking_strategy.py\n  - Processado: crawl4ai/crawl4ai/table_extraction.py\n  - Processado: crawl4ai/crawl4ai/__init__.py\n  - Processado: crawl4ai/crawl4ai/async_url_seeder.py\n  - Processado: crawl4ai/crawl4ai/async_crawler_strategy.py\n  - Processado: crawl4ai/crawl4ai/adaptive_crawler copy.py\n  - Processado: crawl4ai/crawl4ai/ssl_certificate.py\n  - Processado: crawl4ai/crawl4ai/content_filter_strategy.py\n  - Processado: crawl4ai/crawl4ai/user_agent_generator.py\n  - Processado: crawl4ai/crawl4ai/extraction_strategy.py\n  - Processado: crawl4ai/crawl4ai/async_database.py\n  - Processado: crawl4ai/crawl4ai/prompts.py\n  - Processado: crawl4ai/crawl4ai/cli.py\n  - Processado: crawl4ai/crawl4ai/hub.py\n  - Processado: crawl4ai/crawl4ai/link_preview.py\n  - Processado: crawl4ai/crawl4ai/cache_context.py\n  - Processado: crawl4ai/crawl4ai/migrations.py\n  - Processado: crawl4ai/crawl4ai/models.py\n  - Processado: crawl4ai/crawl4ai/adaptive_crawler.py\n  - Processado: crawl4ai/crawl4ai/browser_manager.py\n  - Processado: crawl4ai/crawl4ai/async_configs.py\n  - Processado: crawl4ai/crawl4ai/install.py\n  - Processado: crawl4ai/crawl4ai/utils.py\n  - Processado: crawl4ai/crawl4ai/markdown_generation_strategy.py\n  - Processado: crawl4ai/crawl4ai/async_dispatcher.py\n  - Processado: crawl4ai/crawl4ai/js_snippet/__init__.py\n  - Processado: crawl4ai/crawl4ai/components/crawler_monitor.py\n  - Processado: crawl4ai/crawl4ai/processors/pdf/__init__.py\n  - Processado: crawl4ai/crawl4ai/processors/pdf/processor.py\n  - Processado: crawl4ai/crawl4ai/processors/pdf/utils.py\n  - Processado: crawl4ai/crawl4ai/deep_crawling/crazy.py\n  - Processado: crawl4ai/crawl4ai/deep_crawling/scorers.py\n  - Processado: crawl4ai/crawl4ai/deep_crawling/base_strategy.py\n  - Processado: crawl4ai/crawl4ai/deep_crawling/__init__.py\n  - Processado: crawl4ai/crawl4ai/deep_crawling/filters.py\n  - Processado: crawl4ai/crawl4ai/deep_crawling/dfs_strategy.py\n  - Processado: crawl4ai/crawl4ai/deep_crawling/bfs_strategy.py\n  - Processado: crawl4ai/crawl4ai/deep_crawling/bff_strategy.py\n  - Processado: crawl4ai/crawl4ai/legacy/docs_manager.py\n  - Processado: crawl4ai/crawl4ai/legacy/web_crawler.py\n  - Processado: crawl4ai/crawl4ai/legacy/version_manager.py\n  - Processado: crawl4ai/crawl4ai/legacy/cli.py\n  - Processado: crawl4ai/crawl4ai/legacy/llmtxt.py\n  - Processado: crawl4ai/crawl4ai/legacy/database.py\n  - Processado: crawl4ai/crawl4ai/legacy/crawler_strategy.py\n  - Processado: crawl4ai/crawl4ai/html2text/config.py\n  - Processado: crawl4ai/crawl4ai/html2text/__init__.py\n  - Processado: crawl4ai/crawl4ai/html2text/__main__.py\n  - Processado: crawl4ai/crawl4ai/html2text/elements.py\n  - Processado: crawl4ai/crawl4ai/html2text/cli.py\n  - Processado: crawl4ai/crawl4ai/html2text/_typing.py\n  - Processado: crawl4ai/crawl4ai/html2text/utils.py\n  - Processado: crawl4ai/crawl4ai/script/c4a_compile.py\n  - Processado: crawl4ai/crawl4ai/script/__init__.py\n  - Processado: crawl4ai/crawl4ai/script/c4a_result.py\n  - Processado: crawl4ai/crawl4ai/script/c4ai_script.py\n  - Processado: crawl4ai/crawl4ai/crawlers/google_search/crawler.py\n  - Processado: crawl4ai/crawl4ai/crawlers/amazon_product/crawler.py\n\nAnálise concluída. 70 arquivos .py processados e vetorizados.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nif file_embeddings:\n    # Preparar os dados para o clustering\n    X = np.array(list(file_embeddings.values()))\n    \n    n_clusters = 5\n    \n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n    clusters = kmeans.fit_predict(X)\n    \n    print(\"\\n--- Resultado do Clustering (Inferência de Módulos) ---\")\n    \n    # Mostrar quais arquivos caíram em quais clusters\n    for i in range(n_clusters):\n        print(f\"\\nCluster {i} (Módulo {i}):\")\n        for j, file_path in enumerate(file_paths):\n            if clusters[j] == i:\n                # Imprime o caminho do arquivo relativo ao repositório\n                print(f\"  {file_path.replace('crawl4ai/src/', '')}\")\nelse:\n    print(\"Nenhum arquivo .py foi processado. Clustering não pode ser executado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:29:43.453589Z","iopub.execute_input":"2025-11-10T00:29:43.453827Z","iopub.status.idle":"2025-11-10T00:29:44.072464Z","shell.execute_reply.started":"2025-11-10T00:29:43.453809Z","shell.execute_reply":"2025-11-10T00:29:44.071677Z"}},"outputs":[{"name":"stdout","text":"\n--- Resultado do Clustering (Inferência de Módulos) ---\n\nCluster 0 (Módulo 0):\n  crawl4ai/crawl4ai/legacy/version_manager.py\n  crawl4ai/crawl4ai/html2text/elements.py\n  crawl4ai/crawl4ai/crawlers/amazon_product/crawler.py\n\nCluster 1 (Módulo 1):\n  crawl4ai/crawl4ai/__version__.py\n  crawl4ai/crawl4ai/html2text/__main__.py\n  crawl4ai/crawl4ai/html2text/_typing.py\n\nCluster 2 (Módulo 2):\n  crawl4ai/crawl4ai/browser_adapter.py\n  crawl4ai/crawl4ai/async_logger.py\n  crawl4ai/crawl4ai/proxy_strategy.py\n  crawl4ai/crawl4ai/browser_profiler.py\n  crawl4ai/crawl4ai/docker_client.py\n  crawl4ai/crawl4ai/model_loader.py\n  crawl4ai/crawl4ai/chunking_strategy.py\n  crawl4ai/crawl4ai/table_extraction.py\n  crawl4ai/crawl4ai/ssl_certificate.py\n  crawl4ai/crawl4ai/content_filter_strategy.py\n  crawl4ai/crawl4ai/user_agent_generator.py\n  crawl4ai/crawl4ai/async_database.py\n  crawl4ai/crawl4ai/hub.py\n  crawl4ai/crawl4ai/link_preview.py\n  crawl4ai/crawl4ai/cache_context.py\n  crawl4ai/crawl4ai/migrations.py\n  crawl4ai/crawl4ai/install.py\n  crawl4ai/crawl4ai/markdown_generation_strategy.py\n  crawl4ai/crawl4ai/async_dispatcher.py\n  crawl4ai/crawl4ai/js_snippet/__init__.py\n  crawl4ai/crawl4ai/components/crawler_monitor.py\n  crawl4ai/crawl4ai/processors/pdf/__init__.py\n  crawl4ai/crawl4ai/processors/pdf/utils.py\n  crawl4ai/crawl4ai/deep_crawling/scorers.py\n  crawl4ai/crawl4ai/deep_crawling/base_strategy.py\n  crawl4ai/crawl4ai/deep_crawling/filters.py\n  crawl4ai/crawl4ai/deep_crawling/dfs_strategy.py\n  crawl4ai/crawl4ai/deep_crawling/bfs_strategy.py\n  crawl4ai/crawl4ai/deep_crawling/bff_strategy.py\n  crawl4ai/crawl4ai/legacy/docs_manager.py\n  crawl4ai/crawl4ai/legacy/web_crawler.py\n  crawl4ai/crawl4ai/legacy/cli.py\n  crawl4ai/crawl4ai/legacy/llmtxt.py\n  crawl4ai/crawl4ai/html2text/utils.py\n  crawl4ai/crawl4ai/script/c4a_compile.py\n  crawl4ai/crawl4ai/script/c4ai_script.py\n  crawl4ai/crawl4ai/crawlers/google_search/crawler.py\n\nCluster 3 (Módulo 3):\n  crawl4ai/crawl4ai/config.py\n  crawl4ai/crawl4ai/async_webcrawler.py\n  crawl4ai/crawl4ai/__init__.py\n  crawl4ai/crawl4ai/extraction_strategy.py\n  crawl4ai/crawl4ai/cli.py\n  crawl4ai/crawl4ai/browser_manager.py\n  crawl4ai/crawl4ai/async_configs.py\n  crawl4ai/crawl4ai/deep_crawling/__init__.py\n  crawl4ai/crawl4ai/legacy/database.py\n  crawl4ai/crawl4ai/html2text/__init__.py\n  crawl4ai/crawl4ai/html2text/cli.py\n  crawl4ai/crawl4ai/script/__init__.py\n\nCluster 4 (Módulo 4):\n  crawl4ai/crawl4ai/types.py\n  crawl4ai/crawl4ai/content_scraping_strategy.py\n  crawl4ai/crawl4ai/async_crawler_strategy.back.py\n  crawl4ai/crawl4ai/async_url_seeder.py\n  crawl4ai/crawl4ai/async_crawler_strategy.py\n  crawl4ai/crawl4ai/adaptive_crawler copy.py\n  crawl4ai/crawl4ai/prompts.py\n  crawl4ai/crawl4ai/models.py\n  crawl4ai/crawl4ai/adaptive_crawler.py\n  crawl4ai/crawl4ai/utils.py\n  crawl4ai/crawl4ai/processors/pdf/processor.py\n  crawl4ai/crawl4ai/deep_crawling/crazy.py\n  crawl4ai/crawl4ai/legacy/crawler_strategy.py\n  crawl4ai/crawl4ai/html2text/config.py\n  crawl4ai/crawl4ai/script/c4a_result.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\n\n# Esta função simples limpa os nomes dos arquivos\n# 'async_database.py' -> 'async database'\ndef clean_filename(filename):\n    filename = os.path.basename(filename)\n    filename = filename.replace('.py', '')\n    filename = filename.replace('_', ' ')\n    return filename\n\nprint(\"\\n--- Análise de Tópicos por Cluster (Palavras-Chave) ---\")\n\nif file_embeddings:\n    # Palavras comuns para ignorar\n    # Adicionamos 'init' porque todo pacote python tem\n    stop_words_list = ['init'] \n\n    for i in range(n_clusters):\n        print(f\"\\nCluster {i} (Módulo {i}):\")\n        \n        # 1. Coletar os nomes dos arquivos do cluster\n        cluster_files = []\n        for j, file_path in enumerate(file_paths):\n            if clusters[j] == i:\n                cluster_files.append(file_path)\n        \n        # 2. Limpar os nomes para criar um \"corpus\"\n        corpus = [clean_filename(f) for f in cluster_files]\n        \n        if not corpus:\n            print(\"  (Cluster vazio ou apenas com arquivos __init__.py)\")\n            continue\n            \n        # 3. Usar CountVectorizer para contar as palavras\n        try:\n            vectorizer = CountVectorizer(stop_words=stop_words_list)\n            X = vectorizer.fit_transform(corpus)\n            \n            # 4. Somar a frequência das palavras\n            word_freq = X.sum(axis=0).A1\n            words = vectorizer.get_feature_names_out()\n            \n            # 5. Mostrar as 5 palavras mais comuns\n            top_words = sorted(zip(words, word_freq), key=lambda x: x[1], reverse=True)\n            \n            print(f\"  Top Tópicos: {[word for word, freq in top_words[:5]]}\")\n            \n            # Opcional: mostrar todos os arquivos novamente\n            # for file in corpus:\n            #    print(f\"    - {file}\")\n\n        except ValueError as e:\n            # Isso pode acontecer se o cluster só tiver nomes com stopwords\n            print(f\"  Não foi possível extrair tópicos: {e}\")\n            \nelse:\n    print(\"Nenhum embedding foi processado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T00:29:44.073254Z","iopub.execute_input":"2025-11-10T00:29:44.073611Z","iopub.status.idle":"2025-11-10T00:29:44.118095Z","shell.execute_reply.started":"2025-11-10T00:29:44.073590Z","shell.execute_reply":"2025-11-10T00:29:44.117620Z"}},"outputs":[{"name":"stdout","text":"\n--- Análise de Tópicos por Cluster (Palavras-Chave) ---\n\nCluster 0 (Módulo 0):\n  Top Tópicos: ['crawler', 'elements', 'manager', 'version']\n\nCluster 1 (Módulo 1):\n  Top Tópicos: ['main', 'typing', 'version']\n\nCluster 2 (Módulo 2):\n  Top Tópicos: ['strategy', 'async', 'crawler', 'browser', 'utils']\n\nCluster 3 (Módulo 3):\n  Top Tópicos: ['async', 'cli', 'browser', 'config', 'configs']\n\nCluster 4 (Módulo 4):\n  Top Tópicos: ['crawler', 'strategy', 'async', 'adaptive', 'back']\n","output_type":"stream"}],"execution_count":8}]}