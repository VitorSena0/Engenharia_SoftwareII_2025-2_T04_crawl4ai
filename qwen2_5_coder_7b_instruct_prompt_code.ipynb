{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Ler repositório"
      ],
      "metadata": {
        "id": "4xYU4HQPcRjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/unclecode/crawl4ai.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTa59jfYcNV9",
        "outputId": "82801201-8902-4ffe-972e-4b31a01ab270",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-12T02:04:41.851102Z",
          "iopub.execute_input": "2025-11-12T02:04:41.851429Z",
          "iopub.status.idle": "2025-11-12T02:04:44.323502Z",
          "shell.execute_reply.started": "2025-11-12T02:04:41.851404Z",
          "shell.execute_reply": "2025-11-12T02:04:44.322208Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'crawl4ai'...\n",
            "remote: Enumerating objects: 10157, done.\u001b[K\n",
            "remote: Counting objects: 100% (2492/2492), done.\u001b[K\n",
            "remote: Compressing objects: 100% (529/529), done.\u001b[K\n",
            "remote: Total 10157 (delta 2250), reused 1963 (delta 1963), pack-reused 7665 (from 1)\u001b[K\n",
            "Receiving objects: 100% (10157/10157), 144.05 MiB | 23.05 MiB/s, done.\n",
            "Resolving deltas: 100% (6529/6529), done.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importar token"
      ],
      "metadata": {
        "id": "WmeyJPA0nP-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keggel"
      ],
      "metadata": {
        "id": "MNqiVlPBnW21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar token de acesso do huggingface\n",
        "import os\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "secret_label = \"HF_TOKEN\"\n",
        "secret_value = UserSecretsClient().get_secret(secret_label)\n",
        "\n",
        "\n",
        "os.environ['HF_TOKEN'] = secret_value"
      ],
      "metadata": {
        "id": "FoOwXAYLc43N",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-12T00:57:37.551983Z",
          "iopub.execute_input": "2025-11-12T00:57:37.552333Z",
          "iopub.status.idle": "2025-11-12T00:57:37.599727Z",
          "shell.execute_reply.started": "2025-11-12T00:57:37.552308Z",
          "shell.execute_reply": "2025-11-12T00:57:37.598881Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Colab"
      ],
      "metadata": {
        "id": "iPV1XgMgnc1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "os.environ['HF_TOKEN'] = hf_token"
      ],
      "metadata": {
        "id": "ERotfMZ6nfc5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processar com o modelo 'Qwen/Qwen2.5-Coder-7B-Instruct'\n"
      ],
      "metadata": {
        "id": "0add8f1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir prompt de Sistema"
      ],
      "metadata": {
        "id": "UhD1_6Yvo9bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Conteudo a ser incluido como prompt de sistema\n",
        "system_content = f\"\"\"Você é um Engenheiro de Software de 160 de QI e vai analizar um repositorio e tentar identificar padrões arquiteturais de software nele.\n",
        "\"O que é um padrão arquitetural?\n",
        "\n",
        "Descreve uma solução arquitetural para problemas\n",
        "recorrentes em sistemas software que se\n",
        "apresentam em contextos específicos.\n",
        "\n",
        "A solução arquitetural do padrão define:\n",
        "\n",
        "* as estruturas de software (componentes,\n",
        "serviços, módulos)\n",
        "* responsabilidades e relações entre essas\n",
        "estruturas\n",
        "* formas que as estruturas colaboram para\n",
        "solucionar o problema\"\n",
        "Você vai receber os textos transcritos de arquivos de código do projeto e analizá-los junto com a árvore de diretórios e tentar encontrar estruturas recorrentes na organização do sistema que indiquem o uso de determinados padrões arquiteturais.\n",
        "Você deve informar quais os principais padrões encontrados e dar uma justificativa para cada um.\n",
        "Suas respostas devem ser em português.\n",
        "Para se guiar procure principalmente por indícios das seguintes arquiteturas:\n",
        "*Layers\n",
        "*Pipe-Filter\n",
        "*Cliente-Servidor\n",
        "*Peer-to-Peer\n",
        "*Service-Oriented Arquiteture\n",
        "*Publish-Subscribe\n",
        "*Data-Model\n",
        "*Shared-Data\n",
        "*Microservices\n",
        "*Blackboard\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JtaiFlnoHhbj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-12T00:59:23.649646Z",
          "iopub.execute_input": "2025-11-12T00:59:23.649966Z",
          "iopub.status.idle": "2025-11-12T00:59:23.655764Z",
          "shell.execute_reply.started": "2025-11-12T00:59:23.649945Z",
          "shell.execute_reply": "2025-11-12T00:59:23.654583Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Um prompt para os arquivos mais importantes de uma só vez"
      ],
      "metadata": {
        "id": "65whIlj8pXtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ler conteúdo dos arquivos para inserir no prompt de usuario"
      ],
      "metadata": {
        "id": "gtkHad2rp8yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "IMPORTANT_FILES_ROOT = [\n",
        "    \"Dockerfile\",\n",
        "    \"docker-compose.yml\",\n",
        "    \"pyproject.toml\"\n",
        "]\n",
        "IMPORTANT_FILES_CRAWL = [\n",
        "    \"async_webcrawler.py\",\n",
        "    \"cli.py\",\n",
        "    \"config.py\",\n",
        "    \"extraction_strategy.py\",\n",
        "    \"browser_manager.py\",\n",
        "    \"models.py\",\n",
        "    \"model_loader.py\",\n",
        "    \"async_dispatcher.py\"\n",
        "]\n",
        "IMPORTANT_FILES_DEPLOY_DOCKER = [\n",
        "    \"schemas.py\",\n",
        "    \"server.py\",\n",
        "    \"api.py\"\n",
        "]\n",
        "\n",
        "# string para guardar conteudo dos arquivos\n",
        "total_content = \"Analise os padrões arquiteturais dos seguintes arquivos: \"\n",
        "\n",
        "print(\"Lendo conteudo dos arquivos:\")\n",
        "\n",
        "root_dir = \"crawl4ai\"\n",
        "for file_name in IMPORTANT_FILES_ROOT:\n",
        "  file_path = os.path.join(root_dir, file_name)\n",
        "  try:\n",
        "    with open(file_path, 'r') as f:\n",
        "      content = f.read()\n",
        "      total_content = f\"\\n{file_path}\\n\\nConteúdo:\\n{content}\"\n",
        "  except Exception as e:\n",
        "            print(f\"Could not read {file_path}: {e}\")\n",
        "            pass\n",
        "\n",
        "for file_name in IMPORTANT_FILES_CRAWL:\n",
        "  file_path = os.path.join(root_dir, \"crawl4ai\", file_name)\n",
        "  try:\n",
        "    with open(file_path, 'r') as f:\n",
        "      content = f.read()\n",
        "      total_content = f\"\\n{file_path}\\n\\nConteúdo:\\n{content}\"\n",
        "  except Exception as e:\n",
        "            print(f\"Could not read {file_path}: {e}\")\n",
        "            pass\n",
        "\n",
        "for file_name in IMPORTANT_FILES_DEPLOY_DOCKER:\n",
        "  file_path = os.path.join(root_dir, \"deploy\", \"docker\", file_name)\n",
        "  try:\n",
        "    with open(file_path, 'r') as f:\n",
        "      content = f.read()\n",
        "      total_content = f\"\\n{file_path}\\n\\nConteúdo:\\n{content}\"\n",
        "  except Exception as e:\n",
        "            print(f\"Could not read {file_path}: {e}\")\n",
        "            pass\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-5zembkp4tz",
        "outputId": "40b5af3d-7ea3-4541-8487-be2bdf28473b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lendo conteudo dos arquivos:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rodar o modelo usando Inference Provider do Hugging Face"
      ],
      "metadata": {
        "id": "MkK6sNKmzuvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Inferencia remota via provedores de inferencia\n",
        "client = OpenAI(\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# String para guardar respostas\n",
        "generated_text = \"\"\n",
        "\n",
        "messages_ = [\n",
        "  {\"role\": \"system\", \"content\": system_content},\n",
        "  {\"role\": \"user\", \"content\": total_content},\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Gerar resposta\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "        messages=messages_\n",
        "    )\n",
        "    # Extrair texto gerado\n",
        "    generated_text = response.choices[0].message.content\n",
        "\n",
        "except Exception as e:\n",
        "    generated_text = f\"--- Erro ao processar ---\\n\\n {e}\"\n",
        "\n",
        "\n",
        "# Salvar respostas em um arquivo\n",
        "with open('respostasP1.md', 'a') as f:\n",
        "    file_anwser_content = f\"### --- RESPOSTA PARA O PROMPT COM OS ARQUIVOS MAIS IMPORTANTES ---\\n{generated_text}\\n\\n\"\n",
        "    f.write(file_anwser_content)\n",
        "\n",
        "\n",
        "print(\"Respostas do modelo salvas em 'respostasP1.md'.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZHjMn0Dv2TL",
        "outputId": "52779fd1-a581-4608-bb4c-a0b018a6c3ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respostas do modelo salvas em 'respostasP1.md'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Um prompt para cada arquivo por vez"
      ],
      "metadata": {
        "id": "rx81GDoOpLx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listar os arquivos dentro do repositório 'crawl4ai'\n"
      ],
      "metadata": {
        "id": "5fa82df0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "root_dir = 'crawl4ai'\n",
        "file_paths = []\n",
        "\n",
        "print(f\"Listing files in '{root_dir}':\")\n",
        "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "    for filename in filenames:\n",
        "        full_path = os.path.join(dirpath, filename)\n",
        "        file_paths.append(full_path)\n",
        "        # print(full_path)\n",
        "\n",
        "print(f\"\\nTotal files found: {len(file_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5688d07b",
        "outputId": "7a40fa20-d2de-4bf9-cd43-7a2a6ba2b30c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-12T00:40:02.456408Z",
          "iopub.execute_input": "2025-11-12T00:40:02.456755Z",
          "iopub.status.idle": "2025-11-12T00:40:02.474184Z",
          "shell.execute_reply.started": "2025-11-12T00:40:02.456722Z",
          "shell.execute_reply": "2025-11-12T00:40:02.473400Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing files in 'crawl4ai':\n",
            "\n",
            "Total files found: 739\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ler o conteúdo de cada arquivo identificado no repositório. Priorizaremos arquivos de código para a leitura do modelo.\n"
      ],
      "metadata": {
        "id": "972599c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "TEXT_CODE_EXTENSION = '.py'\n",
        "\n",
        "# Dicionario vazio para guardar conteudo dos arquivos\n",
        "file_contents = {}\n",
        "read_files_count = 0\n",
        "\n",
        "print(\"Lendo conteudo dos arquivos:\")\n",
        "for file_path in file_paths:\n",
        "    # Verficar extensão do arquivo\n",
        "\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "    if file_extension == TEXT_CODE_EXTENSION:\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                content = f.read()\n",
        "                file_contents[file_path] = content\n",
        "                read_files_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Nao foi possivel ler {file_path}: {e}\")\n",
        "            pass\n",
        "\n",
        "# Exibir o número total de arquivos dos quais o conteudo foi lido com sucesso\n",
        "print(f\"\\nConteudo dos arquivos de texto/codigo: {read_files_count}.\")\n",
        "print(f\"{len(file_contents)} armazenados no dicionario 'file_contents'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hludhRH9vGE_",
        "outputId": "40ee18da-e541-490d-ed19-13c55ddbff32",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-12T00:41:38.105060Z",
          "iopub.execute_input": "2025-11-12T00:41:38.105433Z",
          "iopub.status.idle": "2025-11-12T00:41:38.134431Z",
          "shell.execute_reply.started": "2025-11-12T00:41:38.105401Z",
          "shell.execute_reply": "2025-11-12T00:41:38.133287Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lendo conteudo dos arquivos:\n",
            "\n",
            "Conteudo dos arquivos de texto/codigo: 323.\n",
            "323 armazenados no dicionario 'file_contents'.\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rodar o modelo usando Inference Provider do Hugging Face"
      ],
      "metadata": {
        "id": "Ky1WCswM0oQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Inferencia remota via provedores de inferencia\n",
        "client = OpenAI(\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Lista para guardar respostas\n",
        "all_responses = []\n",
        "\n",
        "for nome_arq, conteudo_arq in file_contents.items():\n",
        "    # Criando mensagem para o modelo\n",
        "    prompt_content = f\"Analise os padrões arquiteturais do seguinte arquivo: {nome_arq}\\n\\nConteúdo:\\n{conteudo_arq[:2000]}\"\n",
        "\n",
        "    messages_ = [\n",
        "      {\"role\": \"system\", \"content\": system_content},\n",
        "      {\"role\": \"user\", \"content\": prompt_content},\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Gerar resposta\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
        "            messages=messages_\n",
        "        )\n",
        "        # Extrair texto gerado\n",
        "        generated_text = response.choices[0].message.content\n",
        "        all_responses.append(f\"### --- RESPOSTA PARA {nome_arq} ---\\n{generated_text}\\n\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        all_responses.append(f\"--- Erro ao processar {nome_arq}: {e} ---\\n\\n\")\n",
        "\n",
        "\n",
        "# Salvar respostas em um arquivo\n",
        "with open('respostasP2.md', 'a') as f:\n",
        "    for res in all_responses:\n",
        "        f.write(res)\n",
        "\n",
        "\n",
        "print(f\"Respostas do modelo salvas em 'respostasP2.md'. Arquivos processados: {len(all_responses)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1CIBdr5yc43N",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecba4298-052e-4994-fc65-c99a2ce6ea70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respostas do modelo salvas em 'respostasP2.md'. Arquivos processados: 323\n"
          ]
        }
      ],
      "execution_count": 8
    }
  ]
}